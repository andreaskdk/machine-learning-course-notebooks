{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNTranslation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKraYPnw4Vm5",
        "colab_type": "text"
      },
      "source": [
        "# Translation with Recurrent Neural Networks\n",
        "\n",
        "This notebook is addapted from this notebook: [Seq-2-Seq](https://github.com/random-forests/applied-dl/blob/master/examples/8-seq2seq.ipynb), which in tern is based on this Tensorflow [tutorial](https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention) and this helpful [article](https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "552QcaE33U1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtYXP-576PSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import unicodedata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ENOIoqJNoc",
        "colab_type": "text"
      },
      "source": [
        "The training data is a set of parallel sentences in English and Danish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJVbCgZG9tzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "  (\"Do you want a cup of coffee?\", \"Vil du have en kop kaffe?\"),\n",
        "  (\"I've had coffee already.\", \"Jeg har fået kaffe allerede.\"),\n",
        "  (\"Can I get you a coffee?\", \"Må jeg give dig en kop kaffe?\"),\n",
        "  (\"Please give me some coffee.\", \"Giv mig venligst en kop kaffe.\"),\n",
        "  (\"Would you like me to make coffee?\", \"Skal jeg lave kaffe?\"),\n",
        "  (\"Two coffees, please.\", \"To kaffe, tak.\"),\n",
        "  (\"How about a cup of coffee?\", \"Hvad med en kop kaffe?\"),\n",
        "  (\"I drank two cups of coffee.\", \"Jeg drak to kopper kaffe.\"),\n",
        "  (\"Would you like to have a cup of coffee?\", \"Vil du have en kop kaffe?\"),\n",
        "  (\"There'll be coffee and cake at five.\", \"Der vil være kaffe og kage klokken fem.\"),\n",
        "  (\"Another coffee, please.\", \"Endnu en kaffe, tak.\"),\n",
        "  (\"I made coffee.\", \"Jeg har lavet kaffe.\"),\n",
        "  (\"I would like to have a cup of coffee.\", \"Jeg vil gerne have en kop kaffe.\"),\n",
        "  (\"Do you want me to make coffee?\", \"Skal jeg lave kaffe?\"),\n",
        "  (\"It is hard to wake up without a strong cup of coffee.\", \"Det er svært at vågne uden en kop stærk kaffe.\"),\n",
        "  (\"All I drank was coffee.\", \"Jeg drak kun kaffe.\"),\n",
        "  (\"I've drunk way too much coffee today.\", \"Jeg har drukket for meget kaffe i dag.\"),\n",
        "  (\"Which do you prefer, tea or coffee?\", \"Hvad foretrækker du, te eller kaffe?\"),\n",
        "  (\"There are many kinds of coffee.\", \"Der er mange slags kaffe.\"),\n",
        "  (\"I will make some coffee.\",\t\"Jeg laver noget kaffe.\")\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UstgasohJU1M",
        "colab_type": "text"
      },
      "source": [
        "We need to do some preprocessing of data to handle Danish letters and to make sure that we tokenize correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N20-inhH_rCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(s):\n",
        "  # for details, see https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention\n",
        "  s = re.sub(r\"ø\", r\"oe\", s)\n",
        "  s = re.sub(r\"æ\", r\"ae\", s)\n",
        "  s = re.sub(r\"å\", r\"aa\", s)\n",
        "  s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "  s = re.sub(r\"([?.!,])\", r\" \\1 \", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  s = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", s)\n",
        "  s = s.strip()\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBKycWvF6hrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Original:\", sentences[0])\n",
        "sentences = [(preprocess(source), preprocess(target)) for (source, target) in sentences]\n",
        "print(\"Preprocessed:\", sentences[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSlz7kheKOsQ",
        "colab_type": "text"
      },
      "source": [
        "We tokenize the sentences into words, create a dictionary to be able to replace the words with numbers and add som padding at the end of sentences to make them the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWbWYrUIAdxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_sentences, target_sentences = list(zip(*sentences))\n",
        "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_tokenizer.fit_on_texts(source_sentences)\n",
        "source_data = source_tokenizer.texts_to_sequences(source_sentences)\n",
        "print(\"Sequence:\", source_data[0])\n",
        "source_data = tf.keras.preprocessing.sequence.pad_sequences(source_data, padding='post')\n",
        "print(\"Padded:\", source_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D74TVjT1LNzM",
        "colab_type": "text"
      },
      "source": [
        "We do the same for the target sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB8nZ8TUCGwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_tokenizer.fit_on_texts(target_sentences)\n",
        "target_data = target_tokenizer.texts_to_sequences(target_sentences)\n",
        "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZMMIKBFLKAX",
        "colab_type": "text"
      },
      "source": [
        "Create labels for the decoder by shifting the target sequence one to the right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhWMWEbe05Hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_labels = np.zeros(target_data.shape)\n",
        "target_labels[:,0:target_data.shape[1] -1] = target_data[:,1:]\n",
        "\n",
        "print(\"Target sequence\", target_data[0])\n",
        "print(\"Target label\", target_labels[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8G7XznrGQY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03zIG9MFLdic",
        "colab_type": "text"
      },
      "source": [
        "We define a function to be able to decode the numerical sequence back to a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir6teLD0DVib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode(encoded, tokenizer):\n",
        "  for number in encoded:\n",
        "    if number !=0:\n",
        "      print (\"%d -> %s\" % (number, tokenizer.index_word[number]))\n",
        "      \n",
        "decode(source_data[0], source_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf5yfHccETFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 5\n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels)).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYNaoDqSFSrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_batch = next(iter(dataset))\n",
        "source, target, taget_labels = example_batch\n",
        "print(\"Shapes:\", source.shape, target.shape, taget_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqU008dQFd73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 32\n",
        "rnn_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYJTmXwNLzDr",
        "colab_type": "text"
      },
      "source": [
        "First we define the Encoder. The embedding layer reduces the dimension of the sparse word vector and the GRU layer is the recurrent layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9IBpuqAGAQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(source_vocab_size,\n",
        "                                               embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)        \n",
        "    return output, state\n",
        "  \n",
        "  def init_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, rnn_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3UJBp40K2aq",
        "colab_type": "text"
      },
      "source": [
        "Demonstrate calling the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMjL3nN2GBfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a batch of one sentence\n",
        "ex_sentence = tf.expand_dims(source_data[0], axis=0)\n",
        "ex_translation = tf.expand_dims(target_data[0], axis=0)\n",
        "ex_labels = tf.expand_dims(target_labels[0], axis=0)\n",
        "print(ex_sentence.shape)\n",
        "\n",
        "encoder = Encoder()\n",
        "hidden_state = encoder.init_state(batch_size=1)\n",
        "print(hidden_state.shape)\n",
        "\n",
        "output, hidden_state = encoder(ex_sentence, hidden_state)\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjYF35MMm5B",
        "colab_type": "text"
      },
      "source": [
        "Next we define the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nONasBCIC6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, \n",
        "                                               embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUNFCk-ZVdgT",
        "colab_type": "text"
      },
      "source": [
        "Demonstrate calling the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCiY1TjLMZ1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Decoder()\n",
        "decoder_output, decoder_state = decoder(ex_labels, hidden_state)\n",
        "print(decoder_output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhMLKqtkNA6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def calc_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  return crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "print(\"Loss\", calc_loss(ex_labels, decoder_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS-tPiEGTkVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(idx=None):\n",
        "  \n",
        "    if idx == None: \n",
        "      idx = np.random.choice(len(sentences))\n",
        "    \n",
        "    input_sent = source_data[idx]\n",
        "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "    \n",
        "    hidden_state = encoder.init_state(batch_size=1)\n",
        "    output, hidden_state = encoder(input_sent, hidden_state)\n",
        "    \n",
        "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "    out_words = []\n",
        "    \n",
        "    decoder_state = hidden_state\n",
        "\n",
        "    while True:\n",
        "      \n",
        "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
        "        decoder_input = tf.argmax(decoder_output, -1)\n",
        "        word_idx = decoder_input.numpy()[0][0]\n",
        "        # if we've predicted 0 (which is reserved for the padding, usually \n",
        "        # this will only happen before the decoder is trained, just stop \n",
        "        # translating and return what we have)\n",
        "        if word_idx == 0: \n",
        "          out_words.append('<end>')\n",
        "        else:\n",
        "          out_words.append(target_tokenizer.index_word[word_idx])\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "          break\n",
        "          \n",
        "    translation = ' '.join(out_words)    \n",
        "    return sentences[idx][0], sentences[idx][1], translation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAR0qOfdWQWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_sent, target_sent, translation = translate()\n",
        "print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MUQX_Ltssut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEr-jDSpZ_Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function # remove this annotation when debugging\n",
        "def train_step(source_seq, target_seq, target_labels, initial_state):\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    encoder_output, encoder_state = encoder(source_seq, initial_state)\n",
        "    logits, decoder_state = decoder(target_seq, encoder_state)\n",
        "    loss = calc_loss(target_labels, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn6p-cqiNCa6",
        "colab_type": "text"
      },
      "source": [
        "Do the actual training and print an example every 10th epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjIbuxscstrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 300\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "  \n",
        "    en_initial_states = encoder.init_state(batch_size)\n",
        "    \n",
        "    for batch, (source_seq, target_seq, target_labels) in enumerate(dataset):\n",
        "      loss = train_step(source_seq, target_seq, target_labels, en_initial_states)\n",
        "      elapsed = time.time() - start\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      print(\"Epoch #%d, Loss %.4f, Time %.2f sec\" % (epoch, loss, elapsed))\n",
        "      input_sent, target_sent, translation = translate()\n",
        "      print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtZkjKUX9mxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}