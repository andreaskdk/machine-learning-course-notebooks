{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforcementLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1suifP4mQY5"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "In this exercise we will look at the problem of balancing a stick in one dimension. This is a traditional problem in reinforcement learning (see for example this [video](https://www.youtube.com/watch?v=J7E6_my3CHk)). The objective is to move a cart back and forth while balancing a stick and making sure that the cart does not move too far to either side. A reward is given proportionally to how long the stick is balanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J_iyISzUCUo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "61c51803-e8c7-40b7-dc1f-08d44e9cf769"
      },
      "source": [
        "import IPython\n",
        "IPython.display.HTML(\"\"\"<svg with=400px height=300px version=\"1.1\" viewBox=\"0.0 0.0 399.90288713910763 316.742782152231\" fill=\"none\" stroke=\"none\" stroke-linecap=\"square\" stroke-miterlimit=\"10\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\"><clipPath id=\"p.0\"><path d=\"m0 0l399.9029 0l0 316.74277l-399.9029 0l0 -316.74277z\" clip-rule=\"nonzero\"/></clipPath><g clip-path=\"url(#p.0)\"><path fill=\"#000000\" fill-opacity=\"0.0\" d=\"m0 0l399.9029 0l0 316.74277l-399.9029 0z\" fill-rule=\"evenodd\"/><path fill=\"#000000\" fill-opacity=\"0.0\" d=\"m173.08835 14.829396l16.472443 176.62993\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"12.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m173.08835 14.829396l16.472443 176.62993\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m119.39633 183.67105l154.77164 0l0 41.196854l-154.77164 0z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m119.39633 183.67105l154.77164 0l0 41.196854l-154.77164 0z\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m122.39545 231.3068l0 0c0 -7.227539 7.11409 -13.086624 15.8897705 -13.086624l0 0c8.775665 0 15.889755 5.859085 15.889755 13.086624l0 0c0 7.227539 -7.11409 13.086609 -15.889755 13.086609l0 0c-8.775681 0 -15.8897705 -5.85907 -15.8897705 -13.086609z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m122.39545 231.3068l0 0c0 -7.227539 7.11409 -13.086624 15.8897705 -13.086624l0 0c8.775665 0 15.889755 5.859085 15.889755 13.086624l0 0c0 7.227539 -7.11409 13.086609 -15.889755 13.086609l0 0c-8.775681 0 -15.8897705 -5.85907 -15.8897705 -13.086609z\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m231.48576 231.3068l0 0c0 -7.227539 7.11409 -13.086624 15.889755 -13.086624l0 0c8.775665 0 15.8897705 5.859085 15.8897705 13.086624l0 0c0 7.227539 -7.114105 13.086609 -15.8897705 13.086609l0 0c-8.775665 0 -15.889755 -5.85907 -15.889755 -13.086609z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m231.48576 231.3068l0 0c0 -7.227539 7.11409 -13.086624 15.889755 -13.086624l0 0c8.775665 0 15.8897705 5.859085 15.8897705 13.086624l0 0c0 7.227539 -7.114105 13.086609 -15.8897705 13.086609l0 0c-8.775665 0 -15.889755 -5.85907 -15.889755 -13.086609z\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m297.5687 201.51212l26.188965 0l0 -2.7952728l5.5905457 5.5905457l-5.5905457 5.5905457l0 -2.7952728l-26.188965 0z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m297.5687 201.51212l26.188965 0l0 -2.7952728l5.5905457 5.5905457l-5.5905457 5.5905457l0 -2.7952728l-26.188965 0z\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m95.99792 207.05934l-26.188972 0l0 2.795288l-5.5905533 -5.590561l5.5905533 -5.5905457l0 2.7952728l26.188972 0z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m95.99792 207.05934l-26.188972 0l0 2.795288l-5.5905533 -5.590561l5.5905533 -5.5905457l0 2.7952728l26.188972 0z\" fill-rule=\"evenodd\"/></g></svg>\"\"\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<svg with=400px height=300px version=\"1.1\" viewBox=\"0.0 0.0 399.90288713910763 316.742782152231\" fill=\"none\" stroke=\"none\" stroke-linecap=\"square\" stroke-miterlimit=\"10\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\"><clipPath id=\"p.0\"><path d=\"m0 0l399.9029 0l0 316.74277l-399.9029 0l0 -316.74277z\" clip-rule=\"nonzero\"/></clipPath><g clip-path=\"url(#p.0)\"><path fill=\"#000000\" fill-opacity=\"0.0\" d=\"m0 0l399.9029 0l0 316.74277l-399.9029 0z\" fill-rule=\"evenodd\"/><path fill=\"#000000\" fill-opacity=\"0.0\" d=\"m173.08835 14.829396l16.472443 176.62993\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"12.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m173.08835 14.829396l16.472443 176.62993\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m119.39633 183.67105l154.77164 0l0 41.196854l-154.77164 0z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m119.39633 183.67105l154.77164 0l0 41.196854l-154.77164 0z\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m122.39545 231.3068l0 0c0 -7.227539 7.11409 -13.086624 15.8897705 -13.086624l0 0c8.775665 0 15.889755 5.859085 15.889755 13.086624l0 0c0 7.227539 -7.11409 13.086609 -15.889755 13.086609l0 0c-8.775681 0 -15.8897705 -5.85907 -15.8897705 -13.086609z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m122.39545 231.3068l0 0c0 -7.227539 7.11409 -13.086624 15.8897705 -13.086624l0 0c8.775665 0 15.889755 5.859085 15.889755 13.086624l0 0c0 7.227539 -7.11409 13.086609 -15.889755 13.086609l0 0c-8.775681 0 -15.8897705 -5.85907 -15.8897705 -13.086609z\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m231.48576 231.3068l0 0c0 -7.227539 7.11409 -13.086624 15.889755 -13.086624l0 0c8.775665 0 15.8897705 5.859085 15.8897705 13.086624l0 0c0 7.227539 -7.114105 13.086609 -15.8897705 13.086609l0 0c-8.775665 0 -15.889755 -5.85907 -15.889755 -13.086609z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m231.48576 231.3068l0 0c0 -7.227539 7.11409 -13.086624 15.889755 -13.086624l0 0c8.775665 0 15.8897705 5.859085 15.8897705 13.086624l0 0c0 7.227539 -7.114105 13.086609 -15.8897705 13.086609l0 0c-8.775665 0 -15.889755 -5.85907 -15.889755 -13.086609z\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m297.5687 201.51212l26.188965 0l0 -2.7952728l5.5905457 5.5905457l-5.5905457 5.5905457l0 -2.7952728l-26.188965 0z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m297.5687 201.51212l26.188965 0l0 -2.7952728l5.5905457 5.5905457l-5.5905457 5.5905457l0 -2.7952728l-26.188965 0z\" fill-rule=\"evenodd\"/><path fill=\"#cfe2f3\" d=\"m95.99792 207.05934l-26.188972 0l0 2.795288l-5.5905533 -5.590561l5.5905533 -5.5905457l0 2.7952728l26.188972 0z\" fill-rule=\"evenodd\"/><path stroke=\"#000000\" stroke-width=\"1.0\" stroke-linejoin=\"round\" stroke-linecap=\"butt\" d=\"m95.99792 207.05934l-26.188972 0l0 2.795288l-5.5905533 -5.590561l5.5905533 -5.5905457l0 2.7952728l26.188972 0z\" fill-rule=\"evenodd\"/></g></svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gti8BoKsPWth"
      },
      "source": [
        "The system has a state with four dimensions: position of cart, velocity of the cart, angle of the stick and angular velocity of the stick. At every timestep we have the opportunity to apply a force to the cart in either direction.\n",
        "\n",
        "We start with an initial state that has the cart and stick at rest in the middle and only insert a little bit of randomness to the angle of the stick to get the process running.\n",
        "\n",
        "At each time use the laws of physics (or something fairly close) to calculate the next state of the system. We will let the system run up to a maximum of 200 time steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZANtHSHJe68p"
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def evaluate(action_function):\n",
        "  states=[]\n",
        "  states.append({\"position\": 0.0, \"angle\": random.uniform(-0.1, 0.1), \"carspeed\": 0.0, \"stickspeed\":0.0 })\n",
        "  states[0][\"action\"] = action_function(states[0])\n",
        "  \n",
        "  alpha = 0.1\n",
        "  beta = 0.02\n",
        "  gamma = 0.01\n",
        "  for i in range(200):\n",
        "    if states[i][\"position\"]>-10 and states[i][\"position\"]<10 and states[i][\"angle\"]>-0.5 and states[i][\"angle\"]<0.5:\n",
        "      move=states[i][\"action\"]\n",
        "      states.append({\"position\": states[i][\"position\"]+states[i][\"carspeed\"], \n",
        "                   \"angle\": states[i][\"angle\"]+states[i][\"stickspeed\"], \n",
        "                   \"carspeed\": states[i][\"carspeed\"]+alpha*move, \n",
        "                   \"stickspeed\": states[i][\"stickspeed\"]-math.sin(states[i][\"angle\"])*beta - gamma*move*math.cos(states[i][\"angle\"])})\n",
        "      states[i+1][\"action\"]=action_function(states[i+1])\n",
        "    else:\n",
        "      break\n",
        "      \n",
        "  return states"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebXmrKr1Zo5y"
      },
      "source": [
        "Now, we need a strategy for what actions to take. There are three actions we can choose between at each timestep: negative force (-1), no force (0) and positive force (1). Let us start by having a strategy of choosing between the three options at random.\n",
        "\n",
        "The **evaluate()** function returns the list of states that the system goes through before it stop because the stick falls, the cart moves too far or we reach 200 time steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeaH20vMsQLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce664e5a-5ddb-49f6-e10e-cffc41da9280"
      },
      "source": [
        "def action(state):\n",
        "  return round(random.uniform(-1.5,1.5))\n",
        "  \n",
        "s = evaluate(action)\n",
        "print(len(s))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp3PRyX6cpMX"
      },
      "source": [
        "There is some randomness in how long the stick balances, so we can do it 100 times and see how long the stick is balanced on average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb04S0VI6Owe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a18fc7d-6866-478a-c63f-911ee4edcd4c"
      },
      "source": [
        "def action(state):\n",
        "  return round(random.uniform(-1.5,1.5))\n",
        "\n",
        "rewards=[]\n",
        "for i in range(200):\n",
        "  s=evaluate(action)\n",
        "  rewards.append(len(s))\n",
        "\n",
        "print(sum(rewards)/len(rewards))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDCxyrcTdQMo"
      },
      "source": [
        "# What would be a better action function?\n",
        "\n",
        "Can you think of a better function than randomly choosing actions?\n",
        "\n",
        "One thing we could try is to apply a force in the opposite direction of the angle of the stick."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGgjFHcw1WwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5643a9c-9cf1-4a97-f023-2a48268ea45c"
      },
      "source": [
        "def action(state):\n",
        "  if state[\"angle\"]>0:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1\n",
        "  \n",
        "s = evaluate(action)\n",
        "print(len(s))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLahBShidxJZ"
      },
      "source": [
        "This does not do much better than random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ42j5J24TIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1b3cb8-71dd-434c-cc2b-1afb61966143"
      },
      "source": [
        "def action(state):\n",
        "  if state[\"angle\"]>0:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1\n",
        "\n",
        "rewards=[]\n",
        "for i in range(200):\n",
        "  s=evaluate(action)\n",
        "  rewards.append(len(s))\n",
        "\n",
        "print(sum(rewards)/len(rewards))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3DX-kmRd2_J"
      },
      "source": [
        "Let us now turn to some actual training. We cannot really do supervised learning, since we do not have any pre-labelled data, but we can do something close to that if we start by doing a number of runs with random action and then try to repeat the actions that performed well.\n",
        "\n",
        "We start by doing 500 runs and saving the states along with the reward of the total run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V43m-yXg63yo"
      },
      "source": [
        "# train\n",
        "def action(state):\n",
        "  return round(random.uniform(-1.5,1.5))\n",
        "\n",
        "train_states=[]\n",
        "for i in range(500):\n",
        "  s=evaluate(action)\n",
        "  for t in s:\n",
        "    t[\"reward\"]=len(s)\n",
        "    train_states.append(t)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukIH9GlyjTLm"
      },
      "source": [
        "This gives us ~23.000 states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyuL5iJr-Tkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ffd550-d632-4b0c-d620-29bdb0130a57"
      },
      "source": [
        "len(train_states)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23565"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VYx-U7gkUnL"
      },
      "source": [
        "The training data we can use to define a new action function. For every step, we find the 30 most similar states in the training data and choose the one that gave us the highest reward. This is the action that we repeat. Hence, we exploid the knowledge that we gained in the training phase. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpARaiDK-Vhn"
      },
      "source": [
        "def state_dist(state1, state2):\n",
        "  return abs(state1[\"position\"]-state2[\"position\"]) \\\n",
        "      +abs(state1[\"carspeed\"]-state2[\"carspeed\"]) \\\n",
        "      +abs(state1[\"angle\"]-state2[\"angle\"]) \\\n",
        "      +abs(state1[\"stickspeed\"]-state2[\"stickspeed\"])\n",
        "\n",
        "k=30\n",
        "def action(state):\n",
        "  close=sorted(map(lambda s: {\"dist\": state_dist(s,state), \"reward\": s[\"reward\"], \"action\": s[\"action\"]},train_states), key=lambda x: x[\"dist\"])[0:k]\n",
        "  best=sorted(close, key=lambda x: -x[\"reward\"])[0]\n",
        "  return best[\"action\"]\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeGmfc-hjo0s"
      },
      "source": [
        "And let us see a run with this action function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5Trb982Hq5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f6765a-993d-44fd-b065-2b8ee0feb489"
      },
      "source": [
        "s = evaluate(action)\n",
        "print(len(s))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1lK-LTzag35"
      },
      "source": [
        "# Reinforcement learning\n",
        "\n",
        "It looks better, but we can do better than that with real reinforcement learning. One of the central concepts of reinforcement learning is the trade-off between exploration and exploitation. To begin with we can only explore, because we have no data to exploid yet. So, we introduce an **exploration_rate** that we can decay as we gain more knowledge to exploid.\n",
        "\n",
        "At every timestep, we choose between taking a random action and taking the action that seems to be the most attractive.\n",
        "\n",
        "To see if we learn to balance the stick we print the average reward of the 40 most recent runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g_CmqaiJFct",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4624
        },
        "outputId": "cd8b6c21-2ae4-42cc-f2a5-aebda68ee5e6"
      },
      "source": [
        "exploration_rate=1.0\n",
        "exploration_rate_decay=0.99\n",
        "k=10\n",
        "train_states=[]\n",
        "\n",
        "def action(state):\n",
        "  if len(train_states) > k and random.uniform(0.0, 1.0) > exploration_rate:\n",
        "    close=sorted(map(lambda s: {\"dist\": state_dist(s,state), \"reward\": s[\"reward\"], \"action\": s[\"action\"]},train_states), key=lambda x: x[\"dist\"])[0:k]\n",
        "    best=sorted(close, key=lambda x: -x[\"reward\"])[0]\n",
        "    return best[\"action\"]\n",
        "  else:\n",
        "    return round(random.uniform(-1.5,1.5))\n",
        "  \n",
        "rewards=[]  \n",
        "for i in range(300):\n",
        "  exploration_rate=exploration_rate*exploration_rate_decay\n",
        "  states=evaluate(action)\n",
        "  rewards.append(len(states))\n",
        "  print(sum(rewards[-40:])/len(rewards[-40:]))\n",
        "  for t in states:\n",
        "    t[\"reward\"]=len(states)\n",
        "    train_states.append(t)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62.0\n",
            "66.5\n",
            "60.0\n",
            "52.75\n",
            "49.0\n",
            "49.5\n",
            "52.0\n",
            "51.625\n",
            "54.333333333333336\n",
            "52.9\n",
            "53.0\n",
            "52.583333333333336\n",
            "52.07692307692308\n",
            "51.92857142857143\n",
            "50.4\n",
            "49.375\n",
            "47.529411764705884\n",
            "46.388888888888886\n",
            "45.73684210526316\n",
            "44.9\n",
            "44.19047619047619\n",
            "43.81818181818182\n",
            "43.130434782608695\n",
            "43.666666666666664\n",
            "44.12\n",
            "45.53846153846154\n",
            "45.370370370370374\n",
            "44.964285714285715\n",
            "44.827586206896555\n",
            "44.46666666666667\n",
            "43.774193548387096\n",
            "45.625\n",
            "45.121212121212125\n",
            "45.88235294117647\n",
            "46.68571428571428\n",
            "49.111111111111114\n",
            "48.83783783783784\n",
            "49.526315789473685\n",
            "50.56410256410256\n",
            "50.075\n",
            "49.325\n",
            "49.3\n",
            "49.15\n",
            "49.15\n",
            "49.575\n",
            "50.2\n",
            "49.375\n",
            "48.875\n",
            "48.7\n",
            "49.575\n",
            "49.425\n",
            "49.1\n",
            "50.225\n",
            "50.55\n",
            "51.125\n",
            "51.65\n",
            "52.875\n",
            "53.725\n",
            "56.175\n",
            "57.45\n",
            "58.15\n",
            "58.825\n",
            "60.475\n",
            "61.025\n",
            "60.925\n",
            "60.425\n",
            "60.675\n",
            "63.85\n",
            "65.475\n",
            "65.8\n",
            "68.125\n",
            "68.0\n",
            "68.6\n",
            "67.875\n",
            "67.575\n",
            "65.45\n",
            "65.55\n",
            "65.775\n",
            "64.55\n",
            "64.5\n",
            "65.1\n",
            "65.125\n",
            "65.65\n",
            "66.85\n",
            "68.05\n",
            "68.35\n",
            "68.35\n",
            "68.825\n",
            "67.725\n",
            "66.85\n",
            "67.575\n",
            "68.225\n",
            "68.25\n",
            "67.3\n",
            "67.125\n",
            "68.025\n",
            "69.275\n",
            "68.625\n",
            "66.025\n",
            "65.075\n",
            "65.45\n",
            "65.8\n",
            "65.85\n",
            "65.575\n",
            "65.225\n",
            "65.525\n",
            "65.975\n",
            "63.2\n",
            "62.325\n",
            "62.125\n",
            "59.975\n",
            "60.3\n",
            "61.65\n",
            "63.45\n",
            "63.25\n",
            "63.675\n",
            "64.925\n",
            "64.85\n",
            "66.9\n",
            "67.025\n",
            "68.825\n",
            "68.125\n",
            "67.3\n",
            "69.5\n",
            "68.65\n",
            "69.35\n",
            "69.825\n",
            "72.675\n",
            "74.6\n",
            "74.75\n",
            "74.1\n",
            "74.6\n",
            "73.225\n",
            "73.35\n",
            "73.225\n",
            "72.05\n",
            "71.175\n",
            "71.5\n",
            "75.825\n",
            "77.3\n",
            "77.95\n",
            "77.525\n",
            "78.1\n",
            "79.975\n",
            "80.975\n",
            "80.125\n",
            "79.5\n",
            "80.55\n",
            "80.325\n",
            "82.025\n",
            "82.275\n",
            "82.2\n",
            "82.225\n",
            "81.975\n",
            "81.75\n",
            "81.15\n",
            "80.8\n",
            "80.775\n",
            "79.275\n",
            "79.775\n",
            "78.975\n",
            "80.575\n",
            "80.825\n",
            "78.475\n",
            "77.475\n",
            "76.65\n",
            "77.5\n",
            "75.025\n",
            "75.325\n",
            "76.925\n",
            "77.125\n",
            "78.85\n",
            "79.5\n",
            "80.525\n",
            "82.425\n",
            "84.875\n",
            "84.6\n",
            "86.375\n",
            "82.75\n",
            "83.775\n",
            "85.525\n",
            "85.525\n",
            "85.6\n",
            "83.425\n",
            "84.825\n",
            "85.375\n",
            "86.825\n",
            "88.675\n",
            "88.35\n",
            "89.025\n",
            "89.425\n",
            "88.975\n",
            "87.5\n",
            "88.6\n",
            "88.475\n",
            "89.425\n",
            "89.8\n",
            "88.725\n",
            "92.125\n",
            "92.5\n",
            "93.125\n",
            "91.75\n",
            "95.8\n",
            "94.775\n",
            "95.475\n",
            "96.45\n",
            "95.8\n",
            "96.5\n",
            "94.875\n",
            "94.65\n",
            "94.475\n",
            "91.7\n",
            "92.65\n",
            "93.7\n",
            "94.1\n",
            "93.7\n",
            "95.475\n",
            "96.925\n",
            "99.4\n",
            "98.2\n",
            "96.8\n",
            "97.975\n",
            "97.925\n",
            "98.0\n",
            "96.925\n",
            "96.775\n",
            "95.5\n",
            "95.025\n",
            "98.825\n",
            "99.4\n",
            "100.425\n",
            "99.85\n",
            "100.125\n",
            "98.2\n",
            "101.275\n",
            "100.275\n",
            "99.95\n",
            "100.875\n",
            "100.15\n",
            "102.075\n",
            "101.55\n",
            "101.975\n",
            "97.8\n",
            "100.05\n",
            "100.125\n",
            "99.2\n",
            "101.375\n",
            "102.775\n",
            "104.95\n",
            "104.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a5e50d46b3fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mexploration_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexploration_rate_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ba94e8f89d2b>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(action_function)\u001b[0m\n\u001b[1;32m     14\u001b[0m                    \u001b[0;34m\"carspeed\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"carspeed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                    \"stickspeed\": states[i][\"stickspeed\"]-math.sin(states[i][\"angle\"])*beta - gamma*move*math.cos(states[i][\"angle\"])})\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a5e50d46b3fe>\u001b[0m in \u001b[0;36maction\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mexploration_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"dist\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"action\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dist\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mbest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAYptl4wLwbX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}